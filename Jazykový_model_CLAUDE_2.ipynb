{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+A9Z8rPfL+cFykZO9JOAw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/pokus-cislo/blob/master/Jazykov%C3%BD_model_CLAUDE_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jazykový model CLAUDE 2**"
      ],
      "metadata": {
        "id": "kOIki3ex1GRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monica.im/share/chat?shareId=ZJSufobJSWrDEoCI"
      ],
      "metadata": {
        "id": "xANGFD1j1N82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "FWN2N5xrz3S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE809DW207sz"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/cestina.txt\n",
        "with open('cestina.txt', 'r', encoding='utf-8') as f:\n",
        "    cz_text = f.read()\n",
        "\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/anglictina.txt\n",
        "with open('anglictina.txt', 'r', encoding='utf-8') as f:\n",
        "    en_text = f.read()\n",
        "\n",
        "# Kontrola délky stažených textů\n",
        "print(len(cz_text))\n",
        "print(len(en_text))\n",
        "\n",
        "# Zmenšení datasetu\n",
        "cz_text = cz_text[:10000000]\n",
        "en_text = en_text[:10000000]\n",
        "print(len(cz_text))\n",
        "print(len(en_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizace textů\n",
        "cz_tokens = nltk.word_tokenize(cz_text)\n",
        "en_tokens = nltk.word_tokenize(en_text)\n",
        "\n",
        "# Vytvoření slovníku\n",
        "\n",
        "counter = Counter(cz_tokens + en_tokens)\n",
        "vocab = sorted(counter, key=counter.get, reverse=True)\n",
        "vocab_to_idx = {'<unk>': 0}  # Přidání speciálního tokenu <unk> na začátek slovníku\n",
        "vocab_to_idx.update({token: idx + 1 for idx, token in enumerate(vocab)})\n",
        "\n",
        "# Uložení slovníku do souboru\n",
        "with open(\"vocab_to_idx.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab_to_idx, f)\n",
        "\n",
        "# Převod tokenů na indexy\n",
        "cz_indices = [vocab_to_idx[token] for token in cz_tokens]\n",
        "en_indices = [vocab_to_idx[token] for token in en_tokens]"
      ],
      "metadata": {
        "id": "M8W_k7zv8ZYD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_to_idx)\n",
        "print(f\"\\nvelikost slovniku: {vocab_size}\")\n"
      ],
      "metadata": {
        "id": "C3_5Pnv4pcEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontrola tokenizace\n",
        "print(\"První tokeny z českého textu:\", cz_tokens[:10])\n",
        "print(\"První tokeny z anglického textu:\", en_tokens[:10])\n",
        "\n",
        "# Kontrola slovníku\n",
        "print(\"První tokeny ve slovníku:\")\n",
        "for token, idx in list(vocab_to_idx.items())[:10]:\n",
        "    print(f\"{token}: {idx}\")\n",
        "\n",
        "# Kontrola převodu tokenů na indexy\n",
        "print(\"První indexy z českých indexovaných sekvencí:\", cz_indices[:10])\n",
        "print(\"První indexy z anglických indexovaných sekvencí:\", en_indices[:10])\n"
      ],
      "metadata": {
        "id": "DpEDBAJi-WUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozdělení dat na trénovací, validační a testovací sadu\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "train_size_cz = int(len(cz_indices) * train_ratio)\n",
        "val_size_cz = int(len(cz_indices) * val_ratio)\n",
        "train_size_en = int(len(en_indices) * train_ratio)\n",
        "val_size_en = int(len(en_indices) * val_ratio)\n",
        "\n",
        "train_data_cz = cz_indices[:train_size_cz]\n",
        "val_data_cz = cz_indices[train_size_cz:train_size_cz+val_size_cz]\n",
        "test_data_cz = cz_indices[train_size_cz+val_size_cz:]\n",
        "\n",
        "train_data_en = en_indices[:train_size_en]\n",
        "val_data_en = en_indices[train_size_en:train_size_en+val_size_en]\n",
        "test_data_en = en_indices[train_size_en+val_size_en:]\n",
        "\n",
        "train_data = train_data_cz + train_data_en\n",
        "val_data = val_data_cz + val_data_en\n",
        "test_data = test_data_cz + test_data_en\n",
        "\n",
        "print(\"Délka trénovací sady:\", len(train_data))\n",
        "print(\"Délka validační sady:\", len(val_data))\n",
        "print(\"Délka testovací sady:\", len(test_data))\n"
      ],
      "metadata": {
        "id": "fC7Vj8we_TUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vytvoření datových sad a loaderů\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(0, len(data) - seq_length, seq_length):\n",
        "        seq = data[i:i+seq_length]\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "seq_length = 16  # zmenseno z 32\n",
        "train_sequences = create_sequences(train_data, seq_length)\n",
        "val_sequences = create_sequences(val_data, seq_length)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_sequences, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(val_sequences, dtype=torch.long))\n",
        "\n",
        "batch_size = 128  # zvetseno z 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "V tomto kódu používáme knihovnu nltk pro tokenizaci textů. Vytvoříme slovník mapující unikátní\n",
        "tokeny na jejich indexy pomocí Counter a seřadíme tokeny podle jejich frekvence.\n",
        "Poté převedeme tokeny na jejich indexy pomocí vytvořeného slovníku.\n",
        "Rozdělíme indexované sekvence na trénovací a validační sady v poměru 80% pro trénování a 20% pro validaci.\n",
        "Nakonec vytvoříme datové loadery pomocí DataLoader z PyTorch, které nám umožní snadno procházet daty po dávkách během trénování.\n",
        "Nyní máme připravená data pro trénování našeho jazykového modelu.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "PclWex5U1Pv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Kontrola datových loaderů\n",
        "print(\"Tvar dávky z trénovacího loaderu:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n",
        "\n",
        "print(\"Tvar dávky z validačního loaderu:\")\n",
        "for batch in val_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "eVae48eX1PrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice modelu\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "V této části definujeme architekturu našeho modelu Transformer. Model se skládá z následujících komponent:\n",
        "\n",
        "Embedding vrstva pro převod indexů tokenů na vektory o zadané dimenzi\n",
        "Poziční kódování pro zachycení informací o pozici tokenů v sekvenci\n",
        "Transformer enkodér skládající se z několika vrstev TransformerEncoderLayer\n",
        "Plně propojená vrstva pro převod výstupu enkodéru na pravděpodobnosti přes slovník\n",
        "Dále definujeme třídu PositionalEncoding, která implementuje poziční kódování popsané v článku \"Attention Is All You Need\".\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "FgVGHsKq1Plv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_to_idx)\n",
        "print(f\"\\nvelikost slovniku: {vocab_size}\")"
      ],
      "metadata": {
        "id": "5CN89XvEK1Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializace modelu:\n",
        "\n",
        "# Zmenšení velikosti modelu\n",
        "embedding_dim = 128  # bzlo 256\n",
        "num_heads = 4  # bylo 8\n",
        "num_layers = 2  # bylo 6\n",
        "hidden_dim = 256   # bylo 512\n",
        "dropout = 0.1\n",
        "\n",
        "\"\"\"\n",
        "# Zmenšení délky sekvence\n",
        "seq_length = 16\n",
        "\n",
        "# Zvětšení velikosti dávky\n",
        "batch_size = 128\n",
        "\n",
        "# Zmenšení počtu epoch\n",
        "num_epochs = 3\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "puvodni velikost ...\n",
        "vocab_size = len(vocab_to_idx)\n",
        "embedding_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "hidden_dim = 512\n",
        "dropout = 0.1\n",
        "\"\"\"\n",
        "\n",
        "model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n",
        "\n"
      ],
      "metadata": {
        "id": "S-3qXAHJ1Pe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice ztrátové funkce a optimizátoru:\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Jako ztrátovou funkci používáme křížovou entropii (CrossEntropyLoss) a jako optimizátor používáme Adam s learning rate 0.001."
      ],
      "metadata": {
        "id": "VvhN7cEz1PW-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Trénování modelu:\n",
        "\n",
        "num_epochs = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Zde probíhá trénování modelu po zadaný počet epoch.\n",
        "V každé epoše procházíme trénovací data po dávkách, provádíme dopředný průchod, počítáme ztrátu,\n",
        "provádíme zpětnou propagaci a aktualizujeme parametry modelu pomocí optimizátoru.\n",
        "Na konci každé epochy vypisujeme průměrnou ztrátu.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eiR11vsE1PRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vyhodnocení modelu:\n",
        "\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "print(f\"Validation Loss: {total_loss/len(val_loader):.4f}\")\n",
        "\"\"\"\n",
        "Po trénování vyhodnotíme model na validačních datech.\n",
        "Procházíme validační data po dávkách, provádíme dopředný průchod a počítáme ztrátu.\n",
        "Na konci vypisujeme průměrnou ztrátu na validačních datech.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AtI7IZba1PD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testování modelu\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_data)-seq_length, seq_length):\n",
        "        inputs = torch.tensor(test_data[i:i+seq_length], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        targets = torch.tensor(test_data[i+1:i+seq_length+1], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "test_loss = total_loss / ((len(test_data) - seq_length) / seq_length)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "JltnYa0fWO6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generování textu\n",
        "def generate_text(model, prompt, max_length=100, temperature=0.7):\n",
        "    model.eval()\n",
        "    tokens = nltk.word_tokenize(prompt)\n",
        "    indices = []\n",
        "    for token in tokens:\n",
        "        if token in vocab_to_idx:\n",
        "            indices.append(vocab_to_idx[token])\n",
        "        else:\n",
        "            indices.append(vocab_to_idx['<unk>'])\n",
        "\n",
        "    prompt_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_indices = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(prompt_tensor)\n",
        "            next_token_logits = outputs[-1, -1, :]\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
        "            generated_indices.append(next_token.item())\n",
        "            prompt_tensor = torch.cat((prompt_tensor, next_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "    generated_tokens = [list(vocab_to_idx.keys())[idx] for idx in generated_indices]\n",
        "    generated_text = \" \".join(generated_tokens)\n",
        "    return generated_text\n",
        "\n"
      ],
      "metadata": {
        "id": "3sXdvA4CstMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uložení modelu\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")"
      ],
      "metadata": {
        "id": "YmXFYyRtx-vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zadání promptu a tisk odezvy\n",
        "while True:\n",
        "    prompt = input(\"Zadejte prompt (nebo 'konec' pro ukončení): \")\n",
        "    if prompt.lower() == 'konec':\n",
        "        break\n",
        "\n",
        "    generated_text = generate_text(model, prompt, max_length=100, temperature=0.7)\n",
        "    print(\"Vygenerovaný text:\")\n",
        "    print(generated_text)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnLWeUUzXl3p",
        "outputId": "0f54486e-f50a-4ff2-de11-1199d44deba6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zadejte prompt (nebo 'konec' pro ukončení): KoNeC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Načtení uloženého modelu\n",
        "loaded_model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n",
        "loaded_model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "lkWfOcSY13HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E473gCnyzEoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyBBcUnh13Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbU8BwcF12-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzZvW1aVstB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRvj8UX6ss7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}