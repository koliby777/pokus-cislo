{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbu9psKozvWN/tuSliLSCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/pokus-cislo/blob/master/Jazykov%C3%BD_model_CLAUDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jazykový model CLAUDE**"
      ],
      "metadata": {
        "id": "kOIki3ex1GRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monica.im/share/chat?shareId=QSSCFWegWektmRoB"
      ],
      "metadata": {
        "id": "xANGFD1j1N82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE809DW207sz",
        "outputId": "1765780f-ac4d-4e17-d1af-6e145c207851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 08:52:34--  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/cestina.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97673920 (93M) [text/plain]\n",
            "Saving to: ‘cestina.txt’\n",
            "\n",
            "cestina.txt         100%[===================>]  93.15M   244MB/s    in 0.4s    \n",
            "\n",
            "2024-04-27 08:52:36 (244 MB/s) - ‘cestina.txt’ saved [97673920/97673920]\n",
            "\n",
            "--2024-04-27 08:52:37--  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/anglictina.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94786880 (90M) [text/plain]\n",
            "Saving to: ‘anglictina.txt’\n",
            "\n",
            "anglictina.txt      100%[===================>]  90.40M   178MB/s    in 0.5s    \n",
            "\n",
            "2024-04-27 08:52:38 (178 MB/s) - ‘anglictina.txt’ saved [94786880/94786880]\n",
            "\n",
            "87434531\n",
            "94720742\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/cestina.txt\n",
        "with open('cestina.txt', 'r', encoding='utf-8') as f:\n",
        "    cz_text = f.read()\n",
        "\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/anglictina.txt\n",
        "with open('anglictina.txt', 'r', encoding='utf-8') as f:\n",
        "    en_text = f.read()\n",
        "\n",
        "# Kontrola délky stažených textů\n",
        "print(len(cz_text))\n",
        "print(len(en_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk\n"
      ],
      "metadata": {
        "id": "dIdseeSRGljq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysJTqmjMGzzj",
        "outputId": "d48ac54e-834b-4e60-f533-6d203a47f536"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenizace textů\n",
        "cz_tokens = nltk.word_tokenize(cz_text)\n",
        "en_tokens = nltk.word_tokenize(en_text)\n",
        "\n",
        "# Vytvoření slovníku\n",
        "counter = Counter(cz_tokens + en_tokens)\n",
        "vocab = sorted(counter, key=counter.get, reverse=True)\n",
        "vocab_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "# Převod tokenů na indexy\n",
        "cz_indices = [vocab_to_idx[token] for token in cz_tokens]\n",
        "en_indices = [vocab_to_idx[token] for token in en_tokens]\n",
        "\n",
        "# Vytvoření datových sad\n",
        "train_data = cz_indices[:int(0.8 * len(cz_indices))] + en_indices[:int(0.8 * len(en_indices))]\n",
        "val_data = cz_indices[int(0.8 * len(cz_indices)):] + en_indices[int(0.8 * len(en_indices)):]\n",
        "\n",
        "# Vytvoření datových loaderů\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_data))\n",
        "val_dataset = TensorDataset(torch.tensor(val_data))\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\"\"\"\n",
        "V tomto kódu používáme knihovnu nltk pro tokenizaci textů. Vytvoříme slovník mapující unikátní\n",
        "tokeny na jejich indexy pomocí Counter a seřadíme tokeny podle jejich frekvence.\n",
        "Poté převedeme tokeny na jejich indexy pomocí vytvořeného slovníku.\n",
        "Rozdělíme indexované sekvence na trénovací a validační sady v poměru 80% pro trénování a 20% pro validaci.\n",
        "Nakonec vytvoříme datové loadery pomocí DataLoader z PyTorch, které nám umožní snadno procházet daty po dávkách během trénování.\n",
        "Nyní máme připravená data pro trénování našeho jazykového modelu.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "PclWex5U1Pv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e3e7ad82-16be-43c0-f195-bf4e846350e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nV tomto kódu používáme knihovnu nltk pro tokenizaci textů. Vytvoříme slovník mapující unikátní \\ntokeny na jejich indexy pomocí Counter a seřadíme tokeny podle jejich frekvence. \\nPoté převedeme tokeny na jejich indexy pomocí vytvořeného slovníku.\\nRozdělíme indexované sekvence na trénovací a validační sady v poměru 80% pro trénování a 20% pro validaci. \\nNakonec vytvoříme datové loadery pomocí DataLoader z PyTorch, které nám umožní snadno procházet daty po dávkách během trénování.\\nNyní máme připravená data pro trénování našeho jazykového modelu. \\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontrola tokenizace\n",
        "print(\"První tokeny z českého textu:\", cz_tokens[:10])\n",
        "print(\"První tokeny z anglického textu:\", en_tokens[:10])\n",
        "\n",
        "# Kontrola slovníku\n",
        "print(\"První tokeny ve slovníku:\")\n",
        "for token, idx in list(vocab_to_idx.items())[:10]:\n",
        "    print(f\"{token}: {idx}\")\n",
        "\n",
        "# Kontrola převodu tokenů na indexy\n",
        "print(\"První indexy z českých indexovaných sekvencí:\", cz_indices[:10])\n",
        "print(\"První indexy z anglických indexovaných sekvencí:\", en_indices[:10])\n",
        "\n",
        "# Kontrola datových sad\n",
        "print(\"Délka trénovací sady:\", len(train_data))\n",
        "print(\"Délka validační sady:\", len(val_data))\n",
        "\n",
        "# Kontrola datových loaderů\n",
        "print(\"Tvar dávky z trénovacího loaderu:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n",
        "\n",
        "print(\"Tvar dávky z validačního loaderu:\")\n",
        "for batch in val_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVae48eX1PrI",
        "outputId": "1bf5b4ef-bc5e-49a2-d3c3-9cfd70ac0010"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "První tokeny z českého textu: ['europarl-v7.cs-en.cs', '0000644', '0002002', '0000144', '00566323652', '11662467504', '014035', '0', 'ustar', 'pkoehn']\n",
            "První tokeny z anglického textu: ['ho', 'významu', ',', 'vzhledem', 'k', 'tomu', ',', 'že', '70', '%']\n",
            "První tokeny ve slovníku:\n",
            ",: 0\n",
            ".: 1\n",
            "the: 2\n",
            "a: 3\n",
            "to: 4\n",
            "of: 5\n",
            "and: 6\n",
            "in: 7\n",
            "v: 8\n",
            "that: 9\n",
            "První indexy z českých indexovaných sekvencí: [161876, 125213, 125214, 125215, 161877, 125216, 161878, 18854, 125217, 125218]\n",
            "První indexy z anglických indexovaných sekvencí: [1520, 3441, 0, 1058, 37, 175, 0, 13, 3438, 122]\n",
            "Délka trénovací sady: 25830795\n",
            "Délka validační sady: 6457700\n",
            "Tvar dávky z trénovacího loaderu:\n",
            "torch.Size([64])\n",
            "Tvar dávky z validačního loaderu:\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "V této části definujeme architekturu našeho modelu Transformer. Model se skládá z následujících komponent:\n",
        "\n",
        "Embedding vrstva pro převod indexů tokenů na vektory o zadané dimenzi\n",
        "Poziční kódování pro zachycení informací o pozici tokenů v sekvenci\n",
        "Transformer enkodér skládající se z několika vrstev TransformerEncoderLayer\n",
        "Plně propojená vrstva pro převod výstupu enkodéru na pravděpodobnosti přes slovník\n",
        "Dále definujeme třídu PositionalEncoding, která implementuje poziční kódování popsané v článku \"Attention Is All You Need\".\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "FgVGHsKq1Plv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e3e784fd-c3f5-4ed5-c0c7-5f7bda3883f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nV této části definujeme architekturu našeho modelu Transformer. Model se skládá z následujících komponent:\\n\\nEmbedding vrstva pro převod indexů tokenů na vektory o zadané dimenzi\\nPoziční kódování pro zachycení informací o pozici tokenů v sekvenci\\nTransformer enkodér skládající se z několika vrstev TransformerEncoderLayer\\nPlně propojená vrstva pro převod výstupu enkodéru na pravděpodobnosti přes slovník\\nDále definujeme třídu PositionalEncoding, která implementuje poziční kódování popsané v článku \"Attention Is All You Need\".\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializace modelu:\n",
        "\n",
        "vocab_size = len(vocab_to_idx)\n",
        "embedding_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "hidden_dim = 512\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n"
      ],
      "metadata": {
        "id": "S-3qXAHJ1Pe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96d31d6-b527-4fe6-ddeb-e781893fafdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice ztrátové funkce a optimizátoru:\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Jako ztrátovou funkci používáme křížovou entropii (CrossEntropyLoss) a jako optimizátor používáme Adam s learning rate 0.001."
      ],
      "metadata": {
        "id": "VvhN7cEz1PW-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trénování modelu:\n",
        "\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Zde probíhá trénování modelu po zadaný počet epoch.\n",
        "V každé epoše procházíme trénovací data po dávkách, provádíme dopředný průchod, počítáme ztrátu,\n",
        "provádíme zpětnou propagaci a aktualizujeme parametry modelu pomocí optimizátoru.\n",
        "Na konci každé epochy vypisujeme průměrnou ztrátu.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eiR11vsE1PRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "bef72272-9cb6-4053-dc04-cb68a733c734"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fbbd3573cbe4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vyhodnocení modelu:\n",
        "\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "print(f\"Validation Loss: {total_loss/len(val_loader):.4f}\")\n",
        "\"\"\"\n",
        "Po trénování vyhodnotíme model na validačních datech.\n",
        "Procházíme validační data po dávkách, provádíme dopředný průchod a počítáme ztrátu.\n",
        "Na konci vypisujeme průměrnou ztrátu na validačních datech.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AtI7IZba1PD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sXdvA4CstMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yIGaXPWstIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzZvW1aVstB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRvj8UX6ss7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}