{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/njr+3Um79W1+tDt7c9Eb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/pokus-cislo/blob/master/Jazykov%C3%BD_model_CLAUDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jazykový model CLAUDE**"
      ],
      "metadata": {
        "id": "kOIki3ex1GRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monica.im/share/chat?shareId=QSSCFWegWektmRoB"
      ],
      "metadata": {
        "id": "xANGFD1j1N82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "FWN2N5xrz3S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE809DW207sz"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/cestina.txt\n",
        "with open('cestina.txt', 'r', encoding='utf-8') as f:\n",
        "    cz_text = f.read()\n",
        "\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/anglictina.txt\n",
        "with open('anglictina.txt', 'r', encoding='utf-8') as f:\n",
        "    en_text = f.read()\n",
        "\n",
        "# Kontrola délky stažených textů\n",
        "print(len(cz_text))\n",
        "print(len(en_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk\n"
      ],
      "metadata": {
        "id": "dIdseeSRGljq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "ysJTqmjMGzzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenizace textů\n",
        "cz_tokens = nltk.word_tokenize(cz_text)\n",
        "en_tokens = nltk.word_tokenize(en_text)\n",
        "\n",
        "# Vytvoření slovníku\n",
        "counter = Counter(cz_tokens + en_tokens)\n",
        "vocab = sorted(counter, key=counter.get, reverse=True)\n",
        "vocab_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "# Převod tokenů na indexy\n",
        "cz_indices = [vocab_to_idx[token] for token in cz_tokens]\n",
        "en_indices = [vocab_to_idx[token] for token in en_tokens]\n",
        "\n",
        "# Vytvoření datových sad\n",
        "train_data = cz_indices[:int(0.8 * len(cz_indices))] + en_indices[:int(0.8 * len(en_indices))]\n",
        "val_data = cz_indices[int(0.8 * len(cz_indices)):] + en_indices[int(0.8 * len(en_indices)):]\n",
        "\n",
        "# Vytvoření datových loaderů\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_data))\n",
        "val_dataset = TensorDataset(torch.tensor(val_data))\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\"\"\"\n",
        "V tomto kódu používáme knihovnu nltk pro tokenizaci textů. Vytvoříme slovník mapující unikátní\n",
        "tokeny na jejich indexy pomocí Counter a seřadíme tokeny podle jejich frekvence.\n",
        "Poté převedeme tokeny na jejich indexy pomocí vytvořeného slovníku.\n",
        "Rozdělíme indexované sekvence na trénovací a validační sady v poměru 80% pro trénování a 20% pro validaci.\n",
        "Nakonec vytvoříme datové loadery pomocí DataLoader z PyTorch, které nám umožní snadno procházet daty po dávkách během trénování.\n",
        "Nyní máme připravená data pro trénování našeho jazykového modelu.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "PclWex5U1Pv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontrola tokenizace\n",
        "print(\"První tokeny z českého textu:\", cz_tokens[:10])\n",
        "print(\"První tokeny z anglického textu:\", en_tokens[:10])\n",
        "\n",
        "# Kontrola slovníku\n",
        "print(\"První tokeny ve slovníku:\")\n",
        "for token, idx in list(vocab_to_idx.items())[:10]:\n",
        "    print(f\"{token}: {idx}\")\n",
        "\n",
        "# Kontrola převodu tokenů na indexy\n",
        "print(\"První indexy z českých indexovaných sekvencí:\", cz_indices[:10])\n",
        "print(\"První indexy z anglických indexovaných sekvencí:\", en_indices[:10])\n",
        "\n",
        "# Kontrola datových sad\n",
        "print(\"Délka trénovací sady:\", len(train_data))\n",
        "print(\"Délka validační sady:\", len(val_data))\n",
        "\n",
        "# Kontrola datových loaderů\n",
        "print(\"Tvar dávky z trénovacího loaderu:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n",
        "\n",
        "print(\"Tvar dávky z validačního loaderu:\")\n",
        "for batch in val_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "eVae48eX1PrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice modelu\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "V této části definujeme architekturu našeho modelu Transformer. Model se skládá z následujících komponent:\n",
        "\n",
        "Embedding vrstva pro převod indexů tokenů na vektory o zadané dimenzi\n",
        "Poziční kódování pro zachycení informací o pozici tokenů v sekvenci\n",
        "Transformer enkodér skládající se z několika vrstev TransformerEncoderLayer\n",
        "Plně propojená vrstva pro převod výstupu enkodéru na pravděpodobnosti přes slovník\n",
        "Dále definujeme třídu PositionalEncoding, která implementuje poziční kódování popsané v článku \"Attention Is All You Need\".\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "FgVGHsKq1Plv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializace modelu:\n",
        "\n",
        "vocab_size = len(vocab_to_idx)\n",
        "embedding_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "hidden_dim = 512\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n",
        "\n"
      ],
      "metadata": {
        "id": "S-3qXAHJ1Pe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice ztrátové funkce a optimizátoru:\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Jako ztrátovou funkci používáme křížovou entropii (CrossEntropyLoss) a jako optimizátor používáme Adam s learning rate 0.001."
      ],
      "metadata": {
        "id": "VvhN7cEz1PW-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trénování modelu:\n",
        "\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Zde probíhá trénování modelu po zadaný počet epoch.\n",
        "V každé epoše procházíme trénovací data po dávkách, provádíme dopředný průchod, počítáme ztrátu,\n",
        "provádíme zpětnou propagaci a aktualizujeme parametry modelu pomocí optimizátoru.\n",
        "Na konci každé epochy vypisujeme průměrnou ztrátu.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eiR11vsE1PRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vyhodnocení modelu:\n",
        "\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "print(f\"Validation Loss: {total_loss/len(val_loader):.4f}\")\n",
        "\"\"\"\n",
        "Po trénování vyhodnotíme model na validačních datech.\n",
        "Procházíme validační data po dávkách, provádíme dopředný průchod a počítáme ztrátu.\n",
        "Na konci vypisujeme průměrnou ztrátu na validačních datech.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AtI7IZba1PD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generování textu\n",
        "def generate_text(model, prompt, max_length=100, temperature=0.7):\n",
        "    model.eval()\n",
        "    tokens = nltk.word_tokenize(prompt)\n",
        "    indices = [vocab_to_idx[token] for token in tokens]\n",
        "    prompt_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_indices = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(prompt_tensor)\n",
        "            next_token_logits = outputs[-1, :]\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
        "            generated_indices.append(next_token.item())\n",
        "            prompt_tensor = torch.cat((prompt_tensor, next_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "    generated_tokens = [list(vocab_to_idx.keys())[idx] for idx in generated_indices]\n",
        "    generated_text = \" \".join(generated_tokens)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "3sXdvA4CstMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uložení modelu\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")"
      ],
      "metadata": {
        "id": "8yIGaXPWstIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Načtení uloženého modelu\n",
        "loaded_model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n",
        "loaded_model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "lkWfOcSY13HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyBBcUnh13Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbU8BwcF12-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzZvW1aVstB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRvj8UX6ss7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}