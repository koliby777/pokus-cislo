{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhXlHkfsi+3ckzRMNT0Ttr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/pokus-cislo/blob/master/Jazykov%C3%BD_model_CLAUDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jazykový model CLAUDE**"
      ],
      "metadata": {
        "id": "kOIki3ex1GRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monica.im/share/chat?shareId=WFeFebYlQWFNUbil"
      ],
      "metadata": {
        "id": "xANGFD1j1N82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWN2N5xrz3S-",
        "outputId": "c9ae4753-56ed-4f99-baa6-f1bf46b734ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE809DW207sz",
        "outputId": "3403aada-616f-4706-b87e-50ab803812d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 09:49:37--  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/cestina.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97673920 (93M) [text/plain]\n",
            "Saving to: ‘cestina.txt’\n",
            "\n",
            "cestina.txt         100%[===================>]  93.15M   413MB/s    in 0.2s    \n",
            "\n",
            "2024-04-27 09:49:47 (413 MB/s) - ‘cestina.txt’ saved [97673920/97673920]\n",
            "\n",
            "--2024-04-27 09:49:48--  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/anglictina.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94786880 (90M) [text/plain]\n",
            "Saving to: ‘anglictina.txt’\n",
            "\n",
            "anglictina.txt      100%[===================>]  90.40M   300MB/s    in 0.3s    \n",
            "\n",
            "2024-04-27 09:49:55 (300 MB/s) - ‘anglictina.txt’ saved [94786880/94786880]\n",
            "\n",
            "87434531\n",
            "94720742\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/cestina.txt\n",
        "with open('cestina.txt', 'r', encoding='utf-8') as f:\n",
        "    cz_text = f.read()\n",
        "\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/Texty_EU/anglictina.txt\n",
        "with open('anglictina.txt', 'r', encoding='utf-8') as f:\n",
        "    en_text = f.read()\n",
        "\n",
        "# Kontrola délky stažených textů\n",
        "print(len(cz_text))\n",
        "print(len(en_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizace textů\n",
        "cz_tokens = nltk.word_tokenize(cz_text)\n",
        "en_tokens = nltk.word_tokenize(en_text)\n",
        "\n",
        "# Vytvoření slovníku\n",
        "counter = Counter(cz_tokens + en_tokens)\n",
        "vocab = sorted(counter, key=counter.get, reverse=True)\n",
        "vocab_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "# Převod tokenů na indexy\n",
        "cz_indices = [vocab_to_idx[token] for token in cz_tokens]\n",
        "en_indices = [vocab_to_idx[token] for token in en_tokens]"
      ],
      "metadata": {
        "id": "M8W_k7zv8ZYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontrola tokenizace\n",
        "print(\"První tokeny z českého textu:\", cz_tokens[:10])\n",
        "print(\"První tokeny z anglického textu:\", en_tokens[:10])\n",
        "\n",
        "# Kontrola slovníku\n",
        "print(\"První tokeny ve slovníku:\")\n",
        "for token, idx in list(vocab_to_idx.items())[:10]:\n",
        "    print(f\"{token}: {idx}\")\n",
        "\n",
        "# Kontrola převodu tokenů na indexy\n",
        "print(\"První indexy z českých indexovaných sekvencí:\", cz_indices[:10])\n",
        "print(\"První indexy z anglických indexovaných sekvencí:\", en_indices[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpEDBAJi-WUs",
        "outputId": "d576435d-b5e0-4428-c022-2fb268777618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "První tokeny z českého textu: ['europarl-v7.cs-en.cs', '0000644', '0002002', '0000144', '00566323652', '11662467504', '014035', '0', 'ustar', 'pkoehn']\n",
            "První tokeny z anglického textu: ['ho', 'významu', ',', 'vzhledem', 'k', 'tomu', ',', 'že', '70', '%']\n",
            "První tokeny ve slovníku:\n",
            ",: 0\n",
            ".: 1\n",
            "the: 2\n",
            "a: 3\n",
            "to: 4\n",
            "of: 5\n",
            "and: 6\n",
            "in: 7\n",
            "v: 8\n",
            "that: 9\n",
            "První indexy z českých indexovaných sekvencí: [161876, 125213, 125214, 125215, 161877, 125216, 161878, 18854, 125217, 125218]\n",
            "První indexy z anglických indexovaných sekvencí: [1520, 3441, 0, 1058, 37, 175, 0, 13, 3438, 122]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozdělení dat na trénovací a validační sadu\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(cz_indices) * train_ratio)\n",
        "val_size = len(cz_indices) - train_size\n",
        "\n",
        "train_data_cz = cz_indices[:train_size]\n",
        "val_data_cz = cz_indices[train_size:]\n",
        "\n",
        "train_data_en = en_indices[:train_size]\n",
        "val_data_en = en_indices[train_size:]\n",
        "\n",
        "train_data = train_data_cz + train_data_en\n",
        "val_data = val_data_cz + val_data_en\n",
        "\n",
        "# Kontrola datových sad\n",
        "print(\"Délka trénovací sady:\", len(train_data))\n",
        "print(\"Délka validační sady:\", len(val_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC7Vj8we_TUP",
        "outputId": "6287fc64-1970-4c79-a15c-ca848ff1b6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Délka trénovací sady: 23784608\n",
            "Délka validační sady: 8503887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vytvoření datových sad a loaderů\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(0, len(data) - seq_length, seq_length):\n",
        "        seq = data[i:i+seq_length]\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "seq_length = 16  # zmenseno z 32\n",
        "train_sequences = create_sequences(train_data, seq_length)\n",
        "val_sequences = create_sequences(val_data, seq_length)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(train_sequences, dtype=torch.long))\n",
        "val_dataset = TensorDataset(torch.tensor(val_sequences, dtype=torch.long))\n",
        "\n",
        "batch_size = 128  # zvetseno z 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "V tomto kódu používáme knihovnu nltk pro tokenizaci textů. Vytvoříme slovník mapující unikátní\n",
        "tokeny na jejich indexy pomocí Counter a seřadíme tokeny podle jejich frekvence.\n",
        "Poté převedeme tokeny na jejich indexy pomocí vytvořeného slovníku.\n",
        "Rozdělíme indexované sekvence na trénovací a validační sady v poměru 80% pro trénování a 20% pro validaci.\n",
        "Nakonec vytvoříme datové loadery pomocí DataLoader z PyTorch, které nám umožní snadno procházet daty po dávkách během trénování.\n",
        "Nyní máme připravená data pro trénování našeho jazykového modelu.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "PclWex5U1Pv8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "cc4b0d25-225b-4086-c499-ef53e147b0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nV tomto kódu používáme knihovnu nltk pro tokenizaci textů. Vytvoříme slovník mapující unikátní\\ntokeny na jejich indexy pomocí Counter a seřadíme tokeny podle jejich frekvence.\\nPoté převedeme tokeny na jejich indexy pomocí vytvořeného slovníku.\\nRozdělíme indexované sekvence na trénovací a validační sady v poměru 80% pro trénování a 20% pro validaci.\\nNakonec vytvoříme datové loadery pomocí DataLoader z PyTorch, které nám umožní snadno procházet daty po dávkách během trénování.\\nNyní máme připravená data pro trénování našeho jazykového modelu.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Kontrola datových loaderů\n",
        "print(\"Tvar dávky z trénovacího loaderu:\")\n",
        "for batch in train_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n",
        "\n",
        "print(\"Tvar dávky z validačního loaderu:\")\n",
        "for batch in val_loader:\n",
        "    print(batch[0].shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVae48eX1PrI",
        "outputId": "88426f4c-0fc9-4886-efc4-a838da9ac3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tvar dávky z trénovacího loaderu:\n",
            "torch.Size([128, 16])\n",
            "Tvar dávky z validačního loaderu:\n",
            "torch.Size([128, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice modelu\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "V této části definujeme architekturu našeho modelu Transformer. Model se skládá z následujících komponent:\n",
        "\n",
        "Embedding vrstva pro převod indexů tokenů na vektory o zadané dimenzi\n",
        "Poziční kódování pro zachycení informací o pozici tokenů v sekvenci\n",
        "Transformer enkodér skládající se z několika vrstev TransformerEncoderLayer\n",
        "Plně propojená vrstva pro převod výstupu enkodéru na pravděpodobnosti přes slovník\n",
        "Dále definujeme třídu PositionalEncoding, která implementuje poziční kódování popsané v článku \"Attention Is All You Need\".\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "FgVGHsKq1Plv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e2fa8779-ed5b-43d6-dc12-311a76c8f535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nV této části definujeme architekturu našeho modelu Transformer. Model se skládá z následujících komponent:\\n\\nEmbedding vrstva pro převod indexů tokenů na vektory o zadané dimenzi\\nPoziční kódování pro zachycení informací o pozici tokenů v sekvenci\\nTransformer enkodér skládající se z několika vrstev TransformerEncoderLayer\\nPlně propojená vrstva pro převod výstupu enkodéru na pravděpodobnosti přes slovník\\nDále definujeme třídu PositionalEncoding, která implementuje poziční kódování popsané v článku \"Attention Is All You Need\".\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializace modelu:\n",
        "\n",
        "vocab_size = len(vocab_to_idx)\n",
        "print(f\"\\nvelikost slovniku: {vocab_size}\")\n",
        "\n",
        "# Zmenšení velikosti modelu\n",
        "embedding_dim = 128  # bzlo 256\n",
        "num_heads = 4  # bylo 8\n",
        "num_layers = 2  # bylo 6\n",
        "hidden_dim = 256   # bylo 512\n",
        "dropout = 0.1\n",
        "\n",
        "\"\"\"\n",
        "# Zmenšení délky sekvence\n",
        "seq_length = 16\n",
        "\n",
        "# Zvětšení velikosti dávky\n",
        "batch_size = 128\n",
        "\n",
        "# Zmenšení počtu epoch\n",
        "num_epochs = 3\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "puvodni velikost ...\n",
        "vocab_size = len(vocab_to_idx)\n",
        "embedding_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "hidden_dim = 512\n",
        "dropout = 0.1\n",
        "\"\"\"\n",
        "\n",
        "model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n",
        "\n"
      ],
      "metadata": {
        "id": "S-3qXAHJ1Pe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a788ea0-ebc1-48b5-8a18-3134677eb7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definice ztrátové funkce a optimizátoru:\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Jako ztrátovou funkci používáme křížovou entropii (CrossEntropyLoss) a jako optimizátor používáme Adam s learning rate 0.001."
      ],
      "metadata": {
        "id": "VvhN7cEz1PW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Trénování modelu:\n",
        "\n",
        "num_epochs = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Zde probíhá trénování modelu po zadaný počet epoch.\n",
        "V každé epoše procházíme trénovací data po dávkách, provádíme dopředný průchod, počítáme ztrátu,\n",
        "provádíme zpětnou propagaci a aktualizujeme parametry modelu pomocí optimizátoru.\n",
        "Na konci každé epochy vypisujeme průměrnou ztrátu.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eiR11vsE1PRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e562351-0b61-4b39-ab87-28d0113bd8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vyhodnocení modelu:\n",
        "\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        inputs = batch[0].to(device)\n",
        "        targets = inputs[:, 1:].contiguous()\n",
        "        inputs = inputs[:, :-1].contiguous()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "print(f\"Validation Loss: {total_loss/len(val_loader):.4f}\")\n",
        "\"\"\"\n",
        "Po trénování vyhodnotíme model na validačních datech.\n",
        "Procházíme validační data po dávkách, provádíme dopředný průchod a počítáme ztrátu.\n",
        "Na konci vypisujeme průměrnou ztrátu na validačních datech.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AtI7IZba1PD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generování textu\n",
        "def generate_text(model, prompt, max_length=100, temperature=0.7):\n",
        "    model.eval()\n",
        "    tokens = nltk.word_tokenize(prompt)\n",
        "    indices = [vocab_to_idx[token] for token in tokens]\n",
        "    prompt_tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_indices = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(prompt_tensor)\n",
        "            next_token_logits = outputs[-1, :]\n",
        "            next_token_logits = next_token_logits / temperature\n",
        "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
        "            generated_indices.append(next_token.item())\n",
        "            prompt_tensor = torch.cat((prompt_tensor, next_token.unsqueeze(0)), dim=1)\n",
        "\n",
        "    generated_tokens = [list(vocab_to_idx.keys())[idx] for idx in generated_indices]\n",
        "    generated_text = \" \".join(generated_tokens)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "3sXdvA4CstMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uložení modelu\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")"
      ],
      "metadata": {
        "id": "8yIGaXPWstIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Načtení uloženého modelu\n",
        "loaded_model = TransformerModel(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, dropout)\n",
        "loaded_model.load_state_dict(torch.load(\"transformer_model.pth\"))\n",
        "loaded_model.eval()"
      ],
      "metadata": {
        "id": "lkWfOcSY13HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyBBcUnh13Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TbU8BwcF12-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzZvW1aVstB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRvj8UX6ss7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}