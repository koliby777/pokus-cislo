{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFYWnsQxA9MpqLqeNadVX6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/pokus-cislo/blob/master/Jazykovy_model_2_kytice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pokusy s modelem pro predikci následujícího znaku pomocí transformerů***"
      ],
      "metadata": {
        "id": "GXZv2ynagxxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Co je vlastně ChatGPT?**\n",
        "\n",
        "GPT = Generative Pre-training Transformer (Generativní předtréninkový transformer),\n",
        "\n",
        "LLM = Large Language Model (velký jazykový model),\n",
        "\n",
        "ChatGPT = GPT vyladěný pro povídání si s \"umělou\" inteligencí.\n",
        "\n",
        "Dlaší systémy AI jsou např.: Llama, Claude, Gemini, Mistral, DALL-E ...\n",
        "\n",
        "Obecně : https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view?usp=sharing\n",
        "\n",
        "\n",
        "NN = Neural Network (neuronová síť): https://github.com/koliby777/pokus-cislo/blob/master/EU/neuron.png\n",
        "  https://www.youtube.com/watch?v=IHZwWFHWa-w  \n",
        "\n",
        "Neuronová síť: vstup (otázka/prompt) a výstup (odpověď modelu)\n",
        "\n",
        "NN obecně řečeno vyjadřuje 2 druhy funkci:\n",
        "\n",
        "- fce tranformování vstupu sítě ve výstup sítě (struktura sítě a parametry jedn. neuronů)\n",
        "\n",
        "- fce chyby na výstupu v závislosti na parametrech všech neuronů\n",
        "\n",
        "Výstup je funkcí vstupu  (transformační fce, stochastická)\n",
        "\n",
        "K tomu NN obsahuje pro každý neuron tyto parametry:\n",
        "\n",
        "  - jeho vstupy z předch. vrstvy neuronů\n",
        "\n",
        "  - jeho váhy na vstupu do neuronu\n",
        "\n",
        "  - jeho práh (bias) (\"zkreslení\")\n",
        "\n",
        "  - jeho aktivační funkci (např. sigmoida)\n",
        "\n",
        "  - jeho výstup z neuronu do další vrstvy neuronů\n",
        "Hodnoty vstupů a výstupů z neuronů mají často chrakter pravděpodobnosti (0, 1).\n",
        "\n",
        "\n",
        "Učení modelu (Pre-training):\n",
        "\n",
        "1. parametry všech neuronů se náhodně vygenerují.\n",
        "\n",
        "3. Na přípravu dat pro učení se používá tokenizer. Tokenizer je nástroj zpracování přirozeného jazyka, který rozděluje vstupní text na menší části, nazývané tokeny. Tyto tokeny mohou být slova, fráze, znaky nebo jiné jednotky, na základě kterých se provádí další analýza nebo zpracování textu, včetně kódování na čísla.\n",
        "\n",
        "2. Na vstup celé sítě se pak dávají zakódovaná trénovací data a data validační, na výstupu se srovnávají odezvy sítě, vzniká výstupní chyba (nesoulad v chování modelu mezi trénovacími a validačními daty, training loss vs. validation loss). Chyba je tak velmi složitou funkcí parametrů všech neuronů v mnoharozměrném prostoru.\n",
        "\n",
        "3. Pomocí algoritmu zpětného šíření chyby (backpropagation) se zpětně přepočítávají všechny paramatry modelu až do vstupní vrstvy modelu. Cílem je snížit výstupní chybu. Gradientní sestup vede ke snižování chyby, ideálně k nalezení jejího minima (aspoň s určitou tolerancí).\n",
        "\n",
        "4. To celé (2. až 3.) se např. 10000 krát iteračně opakuje, až je nalezeno jisté minimum chyby. Výsledkem je pak množina parametrů všech neuronů sítě, která zajišťuje přijatelné chování modelu (otázka - odpověď). Odpovědi modelu se dekódují na čitelný text.\n",
        "\n",
        "Používání modelu:\n",
        "- zadání údajů o generování výstupů (počet znaků dílčí dávky odpovědi a počet iterací generování odpovědi)\n",
        "- zadávání otázek (promptů) a získávání odpovědi modelu\n",
        "- po skončení se poslední odpověď modelu přehraje jako řeč\n",
        "\n",
        "\n",
        "\n",
        "******\n",
        "\n",
        "3. Kytice\n",
        "zdroj https://raw.githubusercontent.com/koliby777/pokus-cislo/master/KYTICE/kytice.txt\n",
        "výstup z modelu  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/KYTICE/10x%20KYTICE/10x%20kytice%20vystup%20PC.txt\n",
        "\n",
        "4. Rozmarné léto\n",
        "zdroj https://raw.githubusercontent.com/koliby777/pokus-cislo/master/ROZMARNE%20LETO/rozmarne%20leto.txt\n",
        "výstup z modelu  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/ROZMARNE%20LETO/vystup.txt\n",
        "\n",
        "5. Sborník EU Paralelní korpus 1996 - 2011. https://www.statmt.org/europarl/ European Parliament Proceedings Parallel Corpus 1996-2011. Použity texty v češtině a angličtině 183 MB: 13 mil. slov českých v 669 tis. větách. A 16 mil. slov anglických.\n",
        "Výstup z modelu  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/EU/eu%20trenovani%20%20best.txt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Parametry natrénovaného modelu EU:\n",
        "\n",
        "  Učení trvalo cca 2,5 hodiny, na notebooku Eurocom s GPU NVIDIA GeForce RTX 2080 (Cuda)\n",
        "\n",
        "  Počet parametrů modelu je přes 23 miliónů, které zabírají 108 MB (původní texty EU jsou 183 MB).\n",
        "\n",
        "  To není komprese dat jako např. \"zazipování\", ale o zaznamenání urč. pravděpodobností výskytu slov apod., t.j. model si nepamatuje přesně doslova, co se naučil - zde je podobnost s lidským mozkem (!).\n",
        "\n",
        "  Na zadanou otázku v ČJ či AJ, model odpoví \"co ho napadne\", a to v jazyce otázky (téměř vždy).\n",
        "  \n",
        "  Na stejnou otázku zpravidla dává různé, i když tématicky podobné odpovědi.\n",
        "\n",
        "  Ovšem pro lepší fungování by model musel být podstaně lépe učen: síť by musela být podstatně větší, počet iterací učení rovněž.\n",
        "\n",
        "\n",
        "\n",
        "4. Praktická ukázka  Kytice a EU\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kH0f9c2Vu2pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive\n",
        "!pip install gtts langdetect\n",
        "from gtts import gTTS\n",
        "from langdetect import detect\n",
        "from IPython.display import Audio, display\n",
        "import tempfile\n",
        "import textwrap\n",
        "!pip install gdown\n",
        "import gdown\n",
        "print(\"\\nGPU CUDA je k dispozici: \", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "C22oAIMOlZwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparametry\n",
        "batch_size = 64 # určuje, kolik nezávislých sekvencí bude zpracováváno paralelně\n",
        "block_size = 256 # maximální délka kontextu pro predikce\n",
        "max_iters =   10000 # maximální počet iterací trénování\n",
        "eval_interval = 500 # interval pro evaluaci modelu\n",
        "learning_rate = 3e-4 # rychlost učení\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # zařízení pro výpočty, GPU pokud je dostupné, jinak CPU\n",
        "eval_iters = 200 # počet iterací pro vyhodnocení\n",
        "n_embd = 384 # velikost vektorů vložení\n",
        "n_head = 6 # počet hlav v \"multi-head attention\"\n",
        "\n",
        "n_layer = 11 # počet vrstev transformeru   !!!!!!!! kytice 11, EU 13  !!!!!\n",
        "\n",
        "dropout = 0.2 # pravděpodobnost zapomínání naučeného\n",
        "\n",
        "\n",
        "torch.manual_seed(1337) # nastaví náhodný seed pro reprodukovatelnost\n",
        "\n",
        "\"\"\"\n",
        "# ad EU\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/EU/eu!!!.txt\n",
        "with open('eu!!!.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read() # načte textový soubor k učení\n",
        "\"\"\"\n",
        "# ad Kytice\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/KYTICE/10x%20KYTICE/100x%20kytice.txt\n",
        "with open('100x kytice.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read() # načte textový soubor k učení\n",
        "\n",
        "\n",
        "# zde jsou všechny unikátní znaky, které se v textu vyskytují\n",
        "chars = sorted(list(set(text))) # vytvoří seznam unikátních znaků\n",
        "vocab_size = len(chars) # počet unikátních znaků\n",
        "print(\"----------------------------------------------------\")\n",
        "znaky = ''.join(chars) # spojení všech unikátních znaků do jednoho řetězce bez jakýchkoli oddělovačů\n",
        "zalamovany_text = textwrap.fill(znaky, width=50)\n",
        "print(f\"Model používá těchto {vocab_size} znaků:\\n {zalamovany_text}\") # tisk seznamu unikátních znaků\n",
        "# print(znaky)\n",
        "# print(f\"{len(znaky)} znaků\")\n",
        "\n",
        "# vytvoří mapování znaků na celá čísla\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # mapování znak na index\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # mapování index na znak\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: převede řetězec na seznam čísel\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: převede seznam čísel zpět na řetězec"
      ],
      "metadata": {
        "id": "Rld3z15rJEW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Samotný model\n",
        "\n",
        "# Rozdělení dat na trénovací a validační\n",
        "data = torch.tensor(encode(text), dtype=torch.long) # zakóduje celý text do tensoru\n",
        "n = int(0.9*len(data)) # prvních 90% dat bude trénovacích, zbytek validačních\n",
        "train_data = data[:n] # trénovací data\n",
        "val_data = data[n:] # validační data\n",
        "\n",
        "# načítání dat\n",
        "def get_batch(split):\n",
        "    # vygeneruje malý batch dat pro vstupy x a cíle y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # náhodně vybere začátky sekvencí\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # vytvoří vstupy x\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # vytvoří cíle y (o jednu pozici posunuté)\n",
        "    x, y = x.to(device), y.to(device) # přesune data na zvolené zařízení\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad() # deaktivuje výpočet gradientů pro zvýšení efektivity\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # přepne model do evaluačního módu\n",
        "    for split in ['train', 'val']: # pro trénovací a validační data\n",
        "        losses = torch.zeros(eval_iters) # inicializuje tensor pro ukládání ztrát\n",
        "        for k in range(eval_iters): # pro každou evaluační iteraci\n",
        "            X, Y = get_batch(split) # získá dávku dat\n",
        "            logits, loss = model(X, Y) # vypočítá logity a ztrátu\n",
        "            losses[k] = loss.item() # uloží hodnotu ztráty\n",
        "        out[split] = losses.mean() # průměrná ztráta pro daný split\n",
        "    model.train() # přepne model zpět do trénovacího módu\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module): # definice jedné hlavy self-attention\n",
        "    \"\"\" jedna hlava self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# jednoduchý bigramový model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            logits, loss = self(idx_cond)\n",
        "\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"\\nModel obsahuje \", sum(p.numel() for p in m.parameters())/1e6, 'miliónů parametrů celkem!\\n')\n",
        "# ma byt  23.368986  miliónů parametrů celkem u EU\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "print(\"Optimalizátor vytvořen.\")\n"
      ],
      "metadata": {
        "id": "oB7B1_z3JubL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# učení modelu:\n",
        "\"\"\"\n",
        "for iter in range(max_iters):\n",
        "\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"naučeno !!!!!\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "t9eXqZrinXx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# připojení disku Google:\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7YJeNWWUnQYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uložení parametrů modelu na disk Google:\n",
        "\"\"\"\n",
        "path = '/content/drive/My Drive/MyModels/eu.pth'\n",
        "torch.save(m, path)\n",
        "print(\"\\nModel byl uložen\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Xt0xLJuUk9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# načtení parametrů modelu z disku Google:\n",
        "\"\"\"\n",
        "#ad EU\n",
        "path = '/content/drive/My Drive/MyModels/cs-en.pth'\n",
        "m = torch.load(path, map_location=torch.device(device))\n",
        "print(\"\\nModel byl načten.\\n\")\n",
        "\"\"\"\n",
        "\n",
        "# ad Kytice\n",
        "\"\"\"\n",
        "path = '/content/drive/My Drive/MyModels/100x kytice.pth'\n",
        "m = torch.load(path, map_location=torch.device(device))\n",
        "print(\"\\nModel byl načten.\\n\")\n",
        "\"\"\"\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1scImJYF2hy6J2PVpPvsN92NpkdwRpkDs'\n",
        "output = '100x kytice.pth'\n",
        "\n",
        "# Stažení souboru z Google Drive\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Načtení modelu\n",
        "m = torch.load(output, map_location=torch.device(device))\n"
      ],
      "metadata": {
        "id": "VUVAPsWBkSg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# používání modelu:\n",
        "def dotaz():\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print('Zadej dotaz: (KONEC pro skočení)')\n",
        "    text = input()\n",
        "    return text\n",
        "\n",
        "def zadej_cislo():\n",
        "  t_f = True\n",
        "  chyba = \"to není přirozené číslo!\"\n",
        "  while t_f:\n",
        "    try:\n",
        "      cislo = int(input())\n",
        "      if cislo < 1:\n",
        "        print(chyba)\n",
        "      else:\n",
        "        return cislo\n",
        "        t_f = False\n",
        "    except ValueError:\n",
        "      print(chyba)\n",
        "\n",
        "# Nastavení šířky zalomení textu\n",
        "\n",
        "print(\"----------------------------------------------------\")\n",
        "print('Zadej počet znaků dílčí dávky odpovědi modelu: (10 např.)')\n",
        "partial_batch = zadej_cislo()\n",
        "print(\"----------------------------------------------------\")\n",
        "print('Zadej počet iterací generování odpovědi: (30 např.)')\n",
        "max = zadej_cislo()\n",
        "\n",
        "vstup = \"\"\n",
        "while vstup != \"KONEC\":\n",
        "    vystup = \"\"\n",
        "    initial_text = dotaz()\n",
        "    vstup = initial_text\n",
        "    if vstup == \"KONEC\":\n",
        "      break\n",
        "    ii = 1\n",
        "    while ii <= max:\n",
        "        initial_tokens = encode(initial_text)\n",
        "        context_tensor = torch.tensor([initial_tokens], dtype=torch.long).to(device)  # Přidáváme dimenzi pro dávku\n",
        "        generated_tokens = m.generate(context_tensor, max_new_tokens=partial_batch)  # Generuje zadaný počet tokenů\n",
        "        generated_text = decode(generated_tokens[0, -partial_batch:].tolist())\n",
        "        vystup = vystup + generated_text\n",
        "        initial_text = generated_text\n",
        "        ii += 1\n",
        "\n",
        "    print(\"----------------------------------------------------\")\n",
        "    # print(f\"Výstup: \\n {vystup}\")\n",
        "    # Použití funkce fill() z modulu textwrap pro zalomení textu\n",
        "\n",
        "    \"\"\"\n",
        "    # pro EU\n",
        "    vystup = vystup.replace(\"!\", \" \")\n",
        "    zalamovany_text = textwrap.fill(vystup, width = 70)\n",
        "    \"\"\"\n",
        "\n",
        "    zalamovany_text = vystup # pro Kytici\n",
        "\n",
        "\n",
        "    # Tisk zalomeného textu\n",
        "    print(f\"Výstup: \\n{zalamovany_text}\")\n",
        "\n",
        "lang = detect(zalamovany_text)\n",
        "print(f\"\\nPoslední odezva modelu je v jazyce {lang}:\\n\")\n",
        "tts = gTTS(text=zalamovany_text, lang=lang)\n",
        "with tempfile.NamedTemporaryFile(delete=True, suffix='.mp3') as tmpfile:\n",
        "        tts.save(tmpfile.name)\n",
        "        display(Audio(tmpfile.name, autoplay=True))\n",
        "\n",
        "\n",
        "print(\"...skončeno.\")"
      ],
      "metadata": {
        "id": "4icUhCsLJuUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAa9twFKJuMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wga2iWLEJuFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWxARWQ7Jt5y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}