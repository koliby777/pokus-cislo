{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwwU5cNPqKW8WELCYGeh/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koliby777/pokus-cislo/blob/master/Jazykov%C3%BD_model_Transformer_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jazykový model Transformer**"
      ],
      "metadata": {
        "id": "3Ie3ojh1TiEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://monica.im/share/chat?shareId=CyczXVYLtNuYQpNQ"
      ],
      "metadata": {
        "id": "4oIfuQ-9ThWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Příprava prostředí"
      ],
      "metadata": {
        "id": "_Q6raHzoT3wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Nastavení device na GPU pokud je dostupné, jinak CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Načtení tokenizeru pro multijazyčný BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "print(f'\\nUsing {device} device')"
      ],
      "metadata": {
        "id": "xYaDLlWXT3Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Event. instalace PyTorch"
      ],
      "metadata": {
        "id": "gm0n3FYCUKG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsJoUfagTYwr"
      },
      "outputs": [],
      "source": [
        "# Příkaz pro instalaci PyTorch, pokud je potřeba\n",
        "# !pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Příprava dat"
      ],
      "metadata": {
        "id": "9gj5FDvLULE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ad kytice\n",
        "!wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/KYTICE/10x%20KYTICE/100x%20kytice.txt\n",
        "ggg = '100x kytice.txt'"
      ],
      "metadata": {
        "id": "DhU7nx5weMXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed8ee28-8935-40a4-ff77-c2c39ef5dec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-25 11:31:26--  https://raw.githubusercontent.com/koliby777/pokus-cislo/master/KYTICE/10x%20KYTICE/100x%20kytice.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8069800 (7.7M) [text/plain]\n",
            "Saving to: ‘100x kytice.txt’\n",
            "\n",
            "100x kytice.txt     100%[===================>]   7.70M  36.1MB/s    in 0.2s    \n",
            "\n",
            "2024-04-25 11:31:27 (36.1 MB/s) - ‘100x kytice.txt’ saved [8069800/8069800]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ad EU\n",
        "# !wget https://raw.githubusercontent.com/koliby777/pokus-cislo/master/EU/eu!!!.txt\n",
        "# ggg = 'eu!!!.txt'"
      ],
      "metadata": {
        "id": "BRHg395veMFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funkce pro tokenizaci a přípravu vstupů a segmentů\n",
        "def tokenize_and_prepare_inputs_segments(file_path, max_length=512, segment_length=510):\n",
        "    inputs = {'input_ids': [], 'attention_mask': []}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        words = text.split()\n",
        "        segments = [' '.join(words[i:i+segment_length]) for i in range(0, len(words), segment_length)]\n",
        "\n",
        "        for segment in segments:\n",
        "            segment_inputs = tokenizer(segment, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "            inputs['input_ids'].append(segment_inputs['input_ids'])\n",
        "            inputs['attention_mask'].append(segment_inputs['attention_mask'])\n",
        "\n",
        "    inputs['input_ids'] = torch.cat(inputs['input_ids'], dim=0)\n",
        "    inputs['attention_mask'] = torch.cat(inputs['attention_mask'], dim=0)\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Příklad tokenizace a přípravy vstupů pro český text s rozdělením na segmenty\n",
        "file_path = ggg  # Zde je potřeba upravit cestu k vašemu souboru\n",
        "inputs_segments = tokenize_and_prepare_inputs_segments(file_path)\n",
        "\n",
        "print(f\"Rozměry input_ids: {inputs_segments['input_ids'].shape}\")\n",
        "print(f\"Rozměry attention_mask: {inputs_segments['attention_mask'].shape}\")\n",
        "\n",
        "\"\"\"\n",
        "# Kontrola rozměrů tensorů\n",
        "batch_size = 32\n",
        "assert inputs_segments['input_ids'].shape[0] % batch_size == 0, \"Velikost batche neodpovídá počtu segmentů\"\n",
        "\"\"\"\n",
        "input_ids = inputs_segments['input_ids'].to(device)\n",
        "attention_mask = inputs_segments['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "# Vytvoření tensoru targets posunutím input_ids o 1 doprava\n",
        "# Poslední token každé sekvence je odstraněn a na začátek je přidán token, který může být například [PAD] token nebo jiný speciální token\n",
        "# Zde používáme 0 jako placeholder pro [PAD] token nebo jiný speciální token, který indikuje začátek sekvence\n",
        "targets = torch.cat((input_ids[:, 1:], input_ids[:, :1]*0), dim=1).to(device)\n",
        "\n",
        "# Vytvoření datasetu obsahujícího input_ids, attention_mask (pokud ho máte) a targets\n",
        "# Předpokládáme, že máte tensor attention_mask z předchozího kroku\n",
        "# attention_mask = ...\n",
        "dataset = TensorDataset(input_ids, attention_mask, targets)\n",
        "\n",
        "# Vytvoření DataLoaderu z datasetu\n",
        "batch_size = 32  # Nastavte podle vašich potřeb\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Zde můžete pokračovat s definicí modelu a trénováním\n",
        "\n"
      ],
      "metadata": {
        "id": "RFKGFtYTZa1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  ???????????????? validation\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Předpokládáme, že máte validační input_ids a attention_mask, zatím se rovnají trénovacím datům....\n",
        "# Tady byste měli mít kód pro přípravu validačních input_ids a attention_mask\n",
        "valid_input_ids = inputs_segments['input_ids']\n",
        "valid_attention_mask = inputs_segments['attention_mask']\n",
        "\n",
        "# Vytvoření targets posunutím validačních input_ids o 1 doprava\n",
        "valid_targets = torch.cat((valid_input_ids[:, 1:], valid_input_ids[:, :1]), dim=1)\n",
        "\n",
        "# Vytvoření validačního datasetu\n",
        "valid_dataset = TensorDataset(valid_input_ids, valid_attention_mask, valid_targets)\n",
        "\n",
        "# Vytvoření validačního DataLoaderu\n",
        "valid_batch_size = 32  # Můžete nastavit podle vašich potřeb a možností hardwaru\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=valid_batch_size, shuffle=False)  # Pro evaluaci obvykle nepotřebujeme míchat data\n"
      ],
      "metadata": {
        "id": "Heo-jzW2UQcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def create_mask(input_ids):\n",
        "    # Vytvoření masky pro padding tokeny (předpokládáme, že padding token má ID 0)\n",
        "    return (input_ids != 0).long()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "r1RB6fwmNFtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Vytvoření modelu"
      ],
      "metadata": {
        "id": "k6Gr90GBUL8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "# Nastavení parametrů modelu\n",
        "ntokens = 20000  # velikost slovníku - podle vašeho datasetu - zatím OK , lze i zvětšit, ale tak prodloužit výpočet...\n",
        "emsize = 256  # velikost embeddingů\n",
        "nhid = 256  # velikost skryté vrstvy\n",
        "nlayers = 2  # počet Transformer bloků\n",
        "nhead = 4  # počet hlav v multi-head attention mechanismu\n",
        "dropout = 0.2  # dropout\n",
        "\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
      ],
      "metadata": {
        "id": "Dbk99tYlcCCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Přesunutí tokenizovaných vstupů na správné zařízení\n",
        "input_ids = inputs_segments['input_ids'].to(device)\n",
        "attention_mask = inputs_segments['attention_mask'].to(device)\n"
      ],
      "metadata": {
        "id": "Tap0wuQ_hvK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Trénování modelu"
      ],
      "metadata": {
        "id": "7RqG3TNyUMZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Předpokládáme, že input_ids a attention_mask jsou již připraveny a na správném zařízení\n",
        "\n",
        "# Definice loss funkce a optimalizátoru\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Trénovací cyklus\n",
        "num_epochs = 5  # Nastavte počet epoch podle vašich potřeb\n",
        "\n"
      ],
      "metadata": {
        "id": "5a2I4vWz6VJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for batch in dataloader:\n",
        "        b_input_ids, b_attention_mask, b_targets = batch\n",
        "\n",
        "        b_input_ids = b_input_ids.to(device)\n",
        "        max_index = 19999  # Maximální index pro embedding vrstvu\n",
        "        unk_index = 1  # Předpokládáme, že index 1 reprezentuje [UNK] token\n",
        "        # Nahrazení všech indexů větších než max_index indexem pro [UNK] token\n",
        "        b_input_ids = torch.where(b_input_ids > max_index, unk_index, b_input_ids).to(device)\n",
        "\n",
        "        b_attention_mask = b_attention_mask.to(device)\n",
        "        b_targets = b_targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Generování src_mask pro aktuální batch\n",
        "        seq_length = b_input_ids.size(1)  # Délka sekvence pro aktuální batch\n",
        "        src_mask = model.generate_square_subsequent_mask(seq_length).to(device)\n",
        "        src_mask = src_mask.unsqueeze(0)  # Přidáme rozměr pro batch_size, pokud je potřeba\n",
        "\n",
        "        # Nyní máme src_mask s rozměrem (1, seq_length, seq_length)\n",
        "        # Můžete potřebovat upravit tento kód, aby odpovídal očekávaným rozměrům vaší implementace\n",
        "        output = model(b_input_ids, src_mask)\n",
        "\n",
        "        # Výpočet loss\n",
        "        loss = criterion(output.view(-1, ntokens), b_targets.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Výpis průměrného loss po každé epochě\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    # Tento kód by měl být spuštěn po trénovacím cyklu pro každou epochu nebo po dokončení všech epoch\n",
        "    model.eval()  # Přepnutí modelu do evaluačního režimu\n",
        "    valid_loss = 0.\n",
        "    with torch.no_grad():  # Vypnutí výpočtu gradientů pro evaluaci\n",
        "        for batch in valid_dataloader:\n",
        "            b_input_ids, b_attention_mask, b_targets = batch\n",
        "\n",
        "            b_input_ids = b_input_ids.to(device)\n",
        "            b_attention_mask = b_attention_mask.to(device)\n",
        "            b_targets = b_targets.to(device)\n",
        "\n",
        "            output = model(b_input_ids, b_attention_mask)\n",
        "            loss = criterion(output.view(-1, ntokens), b_targets.view(-1))\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "        print(f\"Validation Loss: {valid_loss / len(valid_dataloader)}\")\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "f699yxcGIVrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max index in b_input_ids:\", b_input_ids.max().item())\n",
        "print(ntokens)\n"
      ],
      "metadata": {
        "id": "l4x8bT84sMLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens in embedding layer:\", model.encoder.weight.size(0))\n"
      ],
      "metadata": {
        "id": "2tMf1R7gssSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluace a použití modelu"
      ],
      "metadata": {
        "id": "JW6_6kr3UM8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zde byste měli přidat kód pro evaluaci modelu a generování textu.\n"
      ],
      "metadata": {
        "id": "I4g8H_LXTuSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 7. Ukládání a načítání modelu"
      ],
      "metadata": {
        "id": "GkusWutQUNUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uložení modelu do Google Drive\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/model_state_dict.pth')\n",
        "\n",
        "# Načtení modelu\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/model_state_dict.pth'))\n"
      ],
      "metadata": {
        "id": "x6-bi-O8TuMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xN139IV4TuFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsJTa_oeTt94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edFyp2DKTt3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}